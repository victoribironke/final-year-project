{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kklc7S7uEUCo"
   },
   "source": [
    "# Predictive Demand Forecasting Using Machine Learning to Reduce Food Waste\n",
    "## Chapter 4: Results and Discussion\n",
    "\n",
    "This notebook trains and evaluates **seven machine-learning models** on Nigerian food-commodity\n",
    "demand data, comparing them with **four evaluation metrics** to identify the best forecasting\n",
    "approach for each crop.\n",
    "\n",
    "### Models Compared (7 models)\n",
    "\n",
    "| # | Model | Family | Key Idea |\n",
    "|---|---|---|---|\n",
    "| 1 | **Random Forest** | Ensemble (bagging) | Averages many independent decision trees |\n",
    "| 2 | **XGBoost** | Ensemble (boosting) | Sequentially corrects errors via gradient descent |\n",
    "| 3 | **SVR** | Kernel-based | Fits an ε-insensitive tube using the RBF kernel |\n",
    "| 4 | **LSTM** | Recurrent Neural Network | Memory cells with input/forget/output gates |\n",
    "| 5 | **GRU** | Recurrent Neural Network | Simplified LSTM with reset & update gates |\n",
    "| 6 | **Prophet** | Statistical (additive) | Decomposes series into trend + seasonality + holidays |\n",
    "| 7 | **N-BEATS** | Deep learning (MLP) | Stacked FC blocks with residual learning |\n",
    "\n",
    "### Evaluation Metrics (4 metrics)\n",
    "\n",
    "| Metric | Full Name | Ideal |\n",
    "|---|---|---|\n",
    "| **RMSE** | Root Mean Squared Error | 0 (lower is better) |\n",
    "| **MAE** | Mean Absolute Error | 0 (lower is better) |\n",
    "| **R²** | Coefficient of Determination | 1.0 (higher is better) |\n",
    "| **MAPE** | Mean Absolute Percentage Error | 0% (lower is better) |\n",
    "\n",
    "**Strategy:** Loop through every unique commodity, train all seven models,\n",
    "evaluate each with all four metrics, and identify the champion model per crop.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6AwR4NpEUCu",
    "outputId": "2456d301-d7a5-4a09-c737-84dc699ded2b"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Install Required Packages\n",
    "# ============================================================\n",
    "!pip install -q xgboost scikit-learn tensorflow pandas numpy matplotlib seaborn tqdm joblib prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qno1igPCEUCw",
    "outputId": "21be98c1-6fce-40b1-95b3-0bffe418b8f6"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Import Libraries\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import joblib, os, json as _json, logging\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# TensorFlow / Keras (LSTM, GRU, N-BEATS)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (LSTM, GRU, Dense, Dropout,\n",
    "                                     Input, Subtract, Add)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Prophet\n",
    "from prophet import Prophet\n",
    "logging.getLogger('prophet').setLevel(logging.ERROR)\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.ERROR)\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "print(f'Pandas version: {pd.__version__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCWXVaWNFP75",
    "outputId": "550d0b7f-6587-4bf7-c14c-62d16fa9736b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "collapsed": true,
    "id": "z8g5I217EUCx",
    "outputId": "67d949ff-38a1-47c5-f888-9d8584fb322b"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Load Data\n",
    "# ============================================================\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/harmonized_food_prices.csv')\n",
    "\n",
    "print(f\"\\nLoaded CSV file with {len(df):,} rows and {len(df.columns)} columns.\")\n",
    "print(f'\\nColumns: {list(df.columns)}')\n",
    "print(f\"\\nDate range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Un1Psx7SEUCy"
   },
   "source": [
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "> **What is EDA?**  Exploratory Data Analysis is the critical first step in any data-science project.\n",
    "> Before building models, we need to *understand* the data — its shape, distributions, missing values,\n",
    "> and relationships between variables.\n",
    "\n",
    "In this section we produce two key visualisations:\n",
    "\n",
    "| Chart | Purpose |\n",
    "|---|---|\n",
    "| **Top Commodities Bar Chart** | Shows which crops appear most often in the dataset. A horizontal bar chart ranks commodities by record count, revealing which ones have enough data to train reliable models. |\n",
    "| **Demand Distribution Histogram** | Displays how demand values are spread. A bell-shaped (normal) distribution is ideal for regression; heavy skew might indicate the need for transformations. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CfvaYzqMEUCz",
    "outputId": "e3bed96f-b4c1-49fb-997c-5eedc1e253b0"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Dataset Overview\n",
    "# ============================================================\n",
    "print('=' * 60)\n",
    "print('DATASET OVERVIEW')\n",
    "print('=' * 60)\n",
    "\n",
    "print(f'\\nShape: {df.shape}')\n",
    "print(f'\\nData Types:')\n",
    "print(df.dtypes)\n",
    "print(f'\\nMissing Values:')\n",
    "print(df.isnull().sum())\n",
    "print(f'\\nBasic Statistics:')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "egS17n0JEUCz",
    "outputId": "764f4d6f-672c-4e44-ef09-60cfc32b1be3"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Commodity Distribution & Demand Overview\n",
    "# ============================================================\n",
    "commodity_counts = df['Commodity_Name'].value_counts()\n",
    "print(f'Unique Commodities: {len(commodity_counts)}\\n')\n",
    "print(commodity_counts.to_string())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top 15 commodities by record count\n",
    "commodity_counts.head(15).plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Top 15 Commodities by Record Count')\n",
    "axes[0].set_xlabel('Number of Records')\n",
    "\n",
    "# Demand distribution\n",
    "df['Demand'].hist(bins=50, ax=axes[1], color='coral', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Demand (Proxy)')\n",
    "axes[1].set_xlabel('Demand Index (0-100)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ograAIXEUC0"
   },
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "**Why preprocess?**\n",
    "Raw data often contains noise and irregularities.  Preprocessing cleans and organises the data so\n",
    "machine-learning models can learn meaningful patterns instead of memorising noise.\n",
    "\n",
    "Steps:\n",
    "1. **Aggregate by Date & Commodity** — Multiple markets report prices for the same commodity on\n",
    "   the same date.  We take the **mean** across all markets to create one clean time series per crop.\n",
    "2. **Forward-fill missing weather data** — Weather observations may have gaps.  Forward-fill carries\n",
    "   the last known value forward; back-fill covers any leading gaps.\n",
    "3. **Sort chronologically** — Time-series models rely on the natural ordering of dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "j6e_3UTGEUC0",
    "outputId": "bd926843-fa39-46b2-b7e0-619c908ed224"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Aggregate by Date & Commodity\n",
    "# ============================================================\n",
    "# Aggregate across all markets for each (Date, Commodity) pair.\n",
    "# This gives us one clean time series per commodity.\n",
    "df_agg = df.groupby(['Date', 'Commodity_Name']).agg({\n",
    "    'Demand': 'mean',\n",
    "    'Unit_Price': 'mean',        # kept for reference only, NOT used as feature\n",
    "    'Avg_Temperature': 'mean',\n",
    "    'Rainfall': 'mean',\n",
    "    'Is_Holiday': 'max'           # 1 if any market flagged it as holiday\n",
    "}).reset_index()\n",
    "\n",
    "df_agg = df_agg.sort_values(['Commodity_Name', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Ensure Date is datetime after groupby\n",
    "df_agg['Date'] = pd.to_datetime(df_agg['Date'])\n",
    "\n",
    "# Forward-fill missing weather data within each commodity group\n",
    "df_agg['Avg_Temperature'] = df_agg.groupby('Commodity_Name')['Avg_Temperature'].transform(\n",
    "    lambda x: x.ffill().bfill()\n",
    ")\n",
    "df_agg['Rainfall'] = df_agg.groupby('Commodity_Name')['Rainfall'].transform(\n",
    "    lambda x: x.ffill().bfill()\n",
    ")\n",
    "\n",
    "print(f'Aggregated dataset: {len(df_agg):,} rows')\n",
    "print(f\"Unique commodities: {df_agg['Commodity_Name'].nunique()}\")\n",
    "print(f'\\nRecords per commodity:')\n",
    "print(df_agg.groupby('Commodity_Name').size().describe())\n",
    "print(f'\\nRemaining missing values:')\n",
    "print(df_agg.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCFiAZqfEUC1"
   },
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "> **What is Feature Engineering?**\n",
    "> Raw fields alone (date, price, temperature) rarely capture the temporal *patterns* in time-series\n",
    "> data.  Feature engineering creates new, more informative variables that help models detect trends,\n",
    "> seasonality, and lagged effects.\n",
    "\n",
    "### Features created\n",
    "\n",
    "| Feature Group | Examples | Why it helps |\n",
    "|---|---|---|\n",
    "| **Lag features** | `Demand_Lag1`, `Lag3`, `Lag6`, `Lag12` | Capture how past demand influences future demand (autoregressive signal). |\n",
    "| **Rolling means** | `RollingMean3`, `RollingMean6`, `RollingMean12` | Smooth out short-term noise and expose the underlying trend. |\n",
    "| **Cyclical month encoding** | `Month_Sin`, `Month_Cos` | Encode the month as a point on a circle so that December (12) is close to January (1). |\n",
    "| **Seasonal indicator** | `Is_Wet_Season` | Nigeria's wet season (April–October) strongly affects agricultural supply and, therefore, demand. |\n",
    "| **Weather variables** | `Avg_Temperature`, `Rainfall` | External climate drivers of crop yield and price. |\n",
    "| **Holiday flag** | `Is_Holiday` | Public holidays may disrupt market activity. |\n",
    "| **Year** | `Year` | Captures long-term inflation or structural market changes. |\n",
    "\n",
    "Creating temporal features for time-series forecasting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "58CRbSR7EUC_",
    "outputId": "3fb52a30-d9b0-4a35-d8b9-a93506b04add"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Feature Engineering Functions\n",
    "# ============================================================\n",
    "\n",
    "def create_features(group_df):\n",
    "    \"\"\"\n",
    "    Create lag features, rolling means, and date features for a single commodity.\n",
    "    Designed for monthly data (each lag unit = 1 month).\n",
    "    \"\"\"\n",
    "    df = group_df.copy()\n",
    "\n",
    "    # --- Lag Features (monthly data) ---\n",
    "    df['Demand_Lag1']  = df['Demand'].shift(1)    # Previous month\n",
    "    df['Demand_Lag3']  = df['Demand'].shift(3)    # 3 months ago\n",
    "    df['Demand_Lag6']  = df['Demand'].shift(6)    # 6 months ago\n",
    "    df['Demand_Lag12'] = df['Demand'].shift(12)   # Same month last year\n",
    "\n",
    "    # --- Rolling Mean Features ---\n",
    "    df['Demand_RollingMean3']  = df['Demand'].rolling(window=3).mean()    # 3-month avg\n",
    "    df['Demand_RollingMean6']  = df['Demand'].rolling(window=6).mean()    # 6-month avg\n",
    "    df['Demand_RollingMean12'] = df['Demand'].rolling(window=12).mean()   # 12-month avg\n",
    "\n",
    "    # --- Date Features ---\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Year']  = df['Date'].dt.year\n",
    "\n",
    "    # Cyclical encoding of month (captures Jan-Dec continuity)\n",
    "    df['Month_Sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_Cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "    # --- Nigerian Seasons ---\n",
    "    # Wet season: April-October, Dry season: November-March\n",
    "    df['Is_Wet_Season'] = df['Month'].apply(lambda m: 1 if 4 <= m <= 10 else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Feature columns the models will train on\n",
    "FEATURE_COLS = [\n",
    "    'Demand_Lag1', 'Demand_Lag3', 'Demand_Lag6', 'Demand_Lag12',\n",
    "    'Demand_RollingMean3', 'Demand_RollingMean6', 'Demand_RollingMean12',\n",
    "    'Avg_Temperature', 'Rainfall', 'Is_Holiday',\n",
    "    'Month_Sin', 'Month_Cos', 'Is_Wet_Season', 'Year'\n",
    "]\n",
    "\n",
    "TARGET_COL = 'Demand'\n",
    "\n",
    "# Minimum samples required to train models for a commodity\n",
    "MIN_SAMPLES = 36   # At least 3 years of monthly data\n",
    "\n",
    "print(f'Feature columns ({len(FEATURE_COLS)}): {FEATURE_COLS}')\n",
    "print(f'Target: {TARGET_COL}')\n",
    "print(f'Minimum samples: {MIN_SAMPLES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tEahLde5EUDA",
    "outputId": "465df454-ad96-4d30-812a-26d7df20b6eb"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Apply Feature Engineering to All Commodities\n",
    "# ============================================================\n",
    "df_features = df_agg.groupby('Commodity_Name', group_keys=False).apply(create_features)\n",
    "\n",
    "# Check which commodities have enough data after dropping NaN rows\n",
    "commodity_sample_counts = {}\n",
    "for commodity in df_features['Commodity_Name'].unique():\n",
    "    crop_data = df_features[df_features['Commodity_Name'] == commodity].dropna(subset=FEATURE_COLS)\n",
    "    commodity_sample_counts[commodity] = len(crop_data)\n",
    "\n",
    "valid_commodities = [c for c, n in commodity_sample_counts.items() if n >= MIN_SAMPLES]\n",
    "skipped_commodities = [c for c, n in commodity_sample_counts.items() if n < MIN_SAMPLES]\n",
    "\n",
    "print(f'Commodities with enough data (>= {MIN_SAMPLES} samples): {len(valid_commodities)}')\n",
    "print(f'Commodities skipped (too few samples): {len(skipped_commodities)}')\n",
    "\n",
    "if skipped_commodities:\n",
    "    print(f'\\nSkipped: {skipped_commodities}')\n",
    "print(f'\\nWill train models for:')\n",
    "for c in valid_commodities:\n",
    "    print(f'  - {c} ({commodity_sample_counts[c]} samples)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc4a_JfqEUDA"
   },
   "source": [
    "## 4. Model Training\n",
    "\n",
    "For each valid commodity:\n",
    "1. **Prepare data**: Extract features, chronological 80/20 train/test split\n",
    "2. **Train Random Forest**: 200 trees, max depth 10\n",
    "3. **Train XGBoost**: 200 rounds, learning rate 0.1\n",
    "4. **Train SVR**: RBF kernel, C=100 (with standard scaling)\n",
    "5. **Train LSTM**: 2-layer LSTM (64 → 32 units), 12-month lookback\n",
    "6. **Train GRU**: 2-layer GRU (64 → 32 units), 12-month lookback\n",
    "7. **Train Prophet**: Facebook’s additive decomposition (trend + seasonality)\n",
    "8. **Train N-BEATS**: Stacked FC blocks with residual learning, 12-month lookback\n",
    "9. **Evaluate**: Compute RMSE, MAE, R², and MAPE for each model\n",
    "\n",
    "### Model Descriptions\n",
    "\n",
    "| Model | Family | How It Works | Strengths |\n",
    "|---|---|---|---|\n",
    "| **Random Forest** | Ensemble (bagging) | Builds many independent decision trees on random subsets of data and averages their predictions. | Robust to outliers, handles non-linear relationships, provides feature importance scores. |\n",
    "| **XGBoost** | Ensemble (boosting) | Builds trees *sequentially* — each new tree corrects the errors of the previous ones using gradient descent. | Often the most accurate on tabular data; built-in regularisation. |\n",
    "| **SVR** | Kernel-based | Fits a curve (via the RBF kernel) that keeps predictions within an ε-margin of the true values. | Effective in high-dimensional spaces; captures non-linear patterns via the kernel trick. |\n",
    "| **LSTM** | Recurrent NN | Processes sequences step-by-step using *memory cells* with input, forget, and output gates. | Captures long-range temporal dependencies. |\n",
    "| **GRU** | Recurrent NN | Simplified LSTM with only two gates (reset & update). | Fewer parameters → faster training; often matches LSTM on shorter sequences. |\n",
    "| **Prophet** | Statistical | Decomposes a time series into **trend** + **seasonality** + **holidays** using a piecewise linear growth curve and Fourier series. | Handles missing data gracefully; built-in holiday support; highly interpretable components. |\n",
    "| **N-BEATS** | Deep Learning (MLP) | Stacks of fully-connected blocks. Each block outputs a *backcast* (reconstructing input) and a *forecast* (predicting future) via residual learning. | Purpose-built for time-series forecasting; no RNN needed; often outperforms LSTM/GRU on benchmarks. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kskmjgXNEUDA",
    "outputId": "dec091df-5546-4b89-e5cb-7865b5cc7b35"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Model Helper Functions\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"Calculate RMSE, MAE, R², and MAPE.\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "    # MAPE - avoid division by zero\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() > 0:\n",
    "        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    else:\n",
    "        mape = np.nan\n",
    "\n",
    "    return {\n",
    "        'RMSE': round(rmse, 4),\n",
    "        'MAE':  round(mae, 4),\n",
    "        'R²':   round(r2, 4),\n",
    "        'MAPE': round(mape, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_lstm_model(n_features, lookback):\n",
    "    \"\"\"Build an LSTM model for time-series forecasting.\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=(lookback, n_features)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_gru_model(n_features, lookback):\n",
    "    \"\"\"Build a GRU model for time-series forecasting.\"\"\"\n",
    "    model = Sequential([\n",
    "        GRU(64, return_sequences=True, input_shape=(lookback, n_features)),\n",
    "        Dropout(0.2),\n",
    "        GRU(32, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_nbeats_model(input_dim, n_stacks=2, n_blocks=3, hidden_dim=128):\n",
    "    \"\"\"\n",
    "    Build a simplified N-BEATS model for demand forecasting.\n",
    "\n",
    "    N-BEATS uses stacks of fully-connected blocks.  Each block outputs a\n",
    "    'backcast' (reconstructing the input) and a 'forecast' (predicting the\n",
    "    target).  The residual from each block feeds the next; all forecasts\n",
    "    are summed.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    residual = inputs\n",
    "    forecast_sum = None\n",
    "\n",
    "    for s in range(n_stacks):\n",
    "        for b in range(n_blocks):\n",
    "            # 4-layer FC block (following the original N-BEATS paper)\n",
    "            h = Dense(hidden_dim, activation='relu')(residual)\n",
    "            h = Dense(hidden_dim, activation='relu')(h)\n",
    "            h = Dense(hidden_dim, activation='relu')(h)\n",
    "            h = Dense(hidden_dim, activation='relu')(h)\n",
    "\n",
    "            # Backcast: reconstruct input (residual learning)\n",
    "            backcast = Dense(input_dim, activation='linear')(h)\n",
    "            # Forecast: predict target\n",
    "            block_forecast = Dense(1, activation='linear')(h)\n",
    "\n",
    "            # Update residual\n",
    "            residual = Subtract()([residual, backcast])\n",
    "\n",
    "            # Accumulate forecast\n",
    "            if forecast_sum is None:\n",
    "                forecast_sum = block_forecast\n",
    "            else:\n",
    "                forecast_sum = Add()([forecast_sum, block_forecast])\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=forecast_sum)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_lstm_sequences(X, y, lookback=12):\n",
    "    \"\"\"Create sliding-window sequences for LSTM / GRU / N-BEATS input.\"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(lookback, len(X)):\n",
    "        Xs.append(X[i - lookback:i])\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "print('Helper functions defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ew3q4hrEUDB",
    "outputId": "3d7b6145-9224-4846-b5ab-8b2d0ba8e3c3"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Per-Crop Training Function\n",
    "# ============================================================\n",
    "\n",
    "def train_and_evaluate_crop(commodity_name, df_features, feature_cols, target_col,\n",
    "                            test_size=0.2, lstm_lookback=12):\n",
    "    \"\"\"\n",
    "    Train RF, XGBoost, SVR, LSTM, GRU, Prophet, and N-BEATS for a single commodity.\n",
    "    Returns dict with model results, predictions, test data, and trained models.\n",
    "    \"\"\"\n",
    "    # Filter and prepare data\n",
    "    crop_data = df_features[df_features['Commodity_Name'] == commodity_name].copy()\n",
    "    crop_data = crop_data.dropna(subset=feature_cols + [target_col])\n",
    "    crop_data = crop_data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    if len(crop_data) < MIN_SAMPLES:\n",
    "        return None\n",
    "\n",
    "    # Chronological train/test split\n",
    "    split_idx = int(len(crop_data) * (1 - test_size))\n",
    "    train_data = crop_data.iloc[:split_idx]\n",
    "    test_data  = crop_data.iloc[split_idx:]\n",
    "\n",
    "    X_train = train_data[feature_cols].values\n",
    "    y_train = train_data[target_col].values\n",
    "    X_test  = test_data[feature_cols].values\n",
    "    y_test  = test_data[target_col].values\n",
    "    test_dates = test_data['Date'].values\n",
    "\n",
    "    results     = {}\n",
    "    predictions = {}\n",
    "    trained_models = {}\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 1. RANDOM FOREST\n",
    "    # -----------------------------------------------------------------\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=10,\n",
    "        min_samples_split=5, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    results['Random Forest'] = evaluate_model(y_test, rf_pred)\n",
    "    predictions['Random Forest'] = rf_pred\n",
    "    trained_models['Random Forest'] = rf_model\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 2. XGBOOST\n",
    "    # -----------------------------------------------------------------\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.1,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42, verbosity=0\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    results['XGBoost'] = evaluate_model(y_test, xgb_pred)\n",
    "    predictions['XGBoost'] = xgb_pred\n",
    "    trained_models['XGBoost'] = xgb_model\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 3. SVR (Support Vector Regression)\n",
    "    # -----------------------------------------------------------------\n",
    "    svr_scaler_X = StandardScaler()\n",
    "    svr_scaler_y = StandardScaler()\n",
    "\n",
    "    X_train_svr = svr_scaler_X.fit_transform(X_train)\n",
    "    y_train_svr = svr_scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "    X_test_svr  = svr_scaler_X.transform(X_test)\n",
    "\n",
    "    svr_model = SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1)\n",
    "    svr_model.fit(X_train_svr, y_train_svr)\n",
    "    svr_pred_scaled = svr_model.predict(X_test_svr)\n",
    "    svr_pred = svr_scaler_y.inverse_transform(svr_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "    results['SVR'] = evaluate_model(y_test, svr_pred)\n",
    "    predictions['SVR'] = svr_pred\n",
    "    trained_models['SVR'] = {\n",
    "        'model': svr_model,\n",
    "        'scaler_X': svr_scaler_X,\n",
    "        'scaler_y': svr_scaler_y,\n",
    "    }\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 4. LSTM\n",
    "    # -----------------------------------------------------------------\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "\n",
    "    X_all = crop_data[feature_cols].values\n",
    "    y_all = crop_data[target_col].values.reshape(-1, 1)\n",
    "\n",
    "    X_scaled = scaler_X.fit_transform(X_all)\n",
    "    y_scaled = scaler_y.fit_transform(y_all).flatten()\n",
    "\n",
    "    X_seq, y_seq = create_lstm_sequences(X_scaled, y_scaled, lookback=lstm_lookback)\n",
    "\n",
    "    if len(X_seq) < MIN_SAMPLES:\n",
    "        results['LSTM'] = {'RMSE': np.nan, 'MAE': np.nan, 'R²': np.nan, 'MAPE': np.nan}\n",
    "        predictions['LSTM'] = np.full(len(y_test), np.nan)\n",
    "    else:\n",
    "        seq_split = split_idx - lstm_lookback\n",
    "        X_seq_train, X_seq_test = X_seq[:seq_split], X_seq[seq_split:]\n",
    "        y_seq_train, y_seq_test = y_seq[:seq_split], y_seq[seq_split:]\n",
    "\n",
    "        lstm_model = build_lstm_model(len(feature_cols), lstm_lookback)\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "        lstm_model.fit(X_seq_train, y_seq_train,\n",
    "                       epochs=100, batch_size=16,\n",
    "                       callbacks=[early_stop], verbose=0)\n",
    "\n",
    "        lstm_pred_scaled = lstm_model.predict(X_seq_test, verbose=0).flatten()\n",
    "        lstm_pred = scaler_y.inverse_transform(lstm_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        y_seq_test_inv = scaler_y.inverse_transform(y_seq_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "        results['LSTM'] = evaluate_model(y_seq_test_inv, lstm_pred)\n",
    "\n",
    "        # Align predictions with test dates\n",
    "        lstm_full = np.full(len(y_test), np.nan)\n",
    "        lstm_full[:len(lstm_pred)] = lstm_pred\n",
    "        predictions['LSTM'] = lstm_full\n",
    "        trained_models['LSTM'] = lstm_model\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 5. GRU\n",
    "    # -----------------------------------------------------------------\n",
    "    if len(X_seq) < MIN_SAMPLES:\n",
    "        results['GRU'] = {'RMSE': np.nan, 'MAE': np.nan, 'R²': np.nan, 'MAPE': np.nan}\n",
    "        predictions['GRU'] = np.full(len(y_test), np.nan)\n",
    "    else:\n",
    "        gru_model = build_gru_model(len(feature_cols), lstm_lookback)\n",
    "        early_stop_gru = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "        gru_model.fit(X_seq_train, y_seq_train,\n",
    "                      epochs=100, batch_size=16,\n",
    "                      callbacks=[early_stop_gru], verbose=0)\n",
    "\n",
    "        gru_pred_scaled = gru_model.predict(X_seq_test, verbose=0).flatten()\n",
    "        gru_pred = scaler_y.inverse_transform(gru_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "        results['GRU'] = evaluate_model(y_seq_test_inv, gru_pred)\n",
    "\n",
    "        gru_full = np.full(len(y_test), np.nan)\n",
    "        gru_full[:len(gru_pred)] = gru_pred\n",
    "        predictions['GRU'] = gru_full\n",
    "        trained_models['GRU'] = gru_model\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 6. PROPHET\n",
    "    # -----------------------------------------------------------------\n",
    "    try:\n",
    "        prophet_data = crop_data[['Date', 'Demand']].copy()\n",
    "        prophet_data.columns = ['ds', 'y']\n",
    "        prophet_train = prophet_data.iloc[:split_idx]\n",
    "        prophet_test  = prophet_data.iloc[split_idx:]\n",
    "\n",
    "        prophet_model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=False,\n",
    "            daily_seasonality=False,\n",
    "            seasonality_mode='multiplicative'\n",
    "        )\n",
    "        prophet_model.fit(prophet_train)\n",
    "\n",
    "        future = prophet_test[['ds']]\n",
    "        forecast = prophet_model.predict(future)\n",
    "        prophet_pred = forecast['yhat'].values\n",
    "\n",
    "        results['Prophet'] = evaluate_model(y_test, prophet_pred)\n",
    "        predictions['Prophet'] = prophet_pred\n",
    "        trained_models['Prophet'] = prophet_model\n",
    "    except Exception as e:\n",
    "        print(f\"    Prophet failed: {e}\")\n",
    "        results['Prophet'] = {'RMSE': np.nan, 'MAE': np.nan, 'R²': np.nan, 'MAPE': np.nan}\n",
    "        predictions['Prophet'] = np.full(len(y_test), np.nan)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # 7. N-BEATS\n",
    "    # -----------------------------------------------------------------\n",
    "    if len(X_seq) < MIN_SAMPLES:\n",
    "        results['N-BEATS'] = {'RMSE': np.nan, 'MAE': np.nan, 'R²': np.nan, 'MAPE': np.nan}\n",
    "        predictions['N-BEATS'] = np.full(len(y_test), np.nan)\n",
    "    else:\n",
    "        # Flatten sequences for N-BEATS (lookback x features -> single vector)\n",
    "        X_nb_train = X_seq_train.reshape(X_seq_train.shape[0], -1)\n",
    "        X_nb_test  = X_seq_test.reshape(X_seq_test.shape[0], -1)\n",
    "\n",
    "        nbeats_model = build_nbeats_model(X_nb_train.shape[1])\n",
    "        early_stop_nb = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "        nbeats_model.fit(X_nb_train, y_seq_train,\n",
    "                         epochs=100, batch_size=16,\n",
    "                         callbacks=[early_stop_nb], verbose=0)\n",
    "\n",
    "        nb_pred_scaled = nbeats_model.predict(X_nb_test, verbose=0).flatten()\n",
    "        nb_pred = scaler_y.inverse_transform(nb_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "        results['N-BEATS'] = evaluate_model(y_seq_test_inv, nb_pred)\n",
    "\n",
    "        nb_full = np.full(len(y_test), np.nan)\n",
    "        nb_full[:len(nb_pred)] = nb_pred\n",
    "        predictions['N-BEATS'] = nb_full\n",
    "        trained_models['N-BEATS'] = nbeats_model\n",
    "\n",
    "    # Feature importance (from Random Forest)\n",
    "    fi = dict(zip(feature_cols, rf_model.feature_importances_))\n",
    "\n",
    "    return {\n",
    "        'results': results,\n",
    "        'predictions': predictions,\n",
    "        'y_test': y_test,\n",
    "        'test_dates': test_dates,\n",
    "        'train_size': len(train_data),\n",
    "        'test_size': len(test_data),\n",
    "        'feature_importance': fi,\n",
    "        'trained_models': trained_models,\n",
    "        'scalers': {'X': scaler_X, 'y': scaler_y},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "DPYKMPSYEUDC",
    "outputId": "95ed19c7-d11f-48ff-f7ae-d96415d35368"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: Run the Multi-Crop Training Loop\n",
    "# ============================================================\n",
    "\n",
    "all_results = {}\n",
    "all_crop_outputs = {}\n",
    "\n",
    "print('=' * 70)\n",
    "print('MULTI-CROP MODEL TRAINING')\n",
    "print('=' * 70)\n",
    "\n",
    "for i, commodity in enumerate(valid_commodities):\n",
    "    print(f\"\\n{chr(9472) * 70}\")\n",
    "    print(f'[{i+1}/{len(valid_commodities)}] Training models for: {commodity}')\n",
    "    print(f\"{chr(9472) * 70}\")\n",
    "\n",
    "    output = train_and_evaluate_crop(\n",
    "        commodity_name=commodity,\n",
    "        df_features=df_features,\n",
    "        feature_cols=FEATURE_COLS,\n",
    "        target_col=TARGET_COL\n",
    "    )\n",
    "\n",
    "    if output is None:\n",
    "        print(f'  Skipped (insufficient data)')\n",
    "        continue\n",
    "\n",
    "    all_crop_outputs[commodity] = output\n",
    "    all_results[commodity] = output['results']\n",
    "\n",
    "    # Print results for this commodity\n",
    "    print(f\"  Train: {output['train_size']} samples | Test: {output['test_size']} samples\")\n",
    "    for model_name, metrics in output['results'].items():\n",
    "        print(f\"  {model_name:15s} -> RMSE: {metrics['RMSE']:8.4f} | MAE: {metrics['MAE']:8.4f} | R²: {metrics['R²']:7.4f} | MAPE: {metrics['MAPE']:6.2f}%\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f'TRAINING COMPLETE: {len(all_results)} commodities processed')\n",
    "print(f\"{'=' * 70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Av8z4PEyEUDC"
   },
   "source": [
    "## 5. Results & Visualisation\n",
    "\n",
    "Now that all seven models have been trained on every valid commodity, we visualise the results to\n",
    "understand which models perform best and why.\n",
    "\n",
    "### 5.1 Actual vs. Forecast Plots\n",
    "\n",
    "> **How to read this chart:**\n",
    "> - The **black line with dots** represents the *actual* demand values from the test set.\n",
    "> - Each **coloured dashed line** represents a model’s *predicted* values.\n",
    "> - The closer a coloured line follows the black line, the better that model is at capturing the\n",
    ">   true demand pattern.\n",
    "> - Large gaps between a model’s line and the actual line indicate periods where the model struggled\n",
    ">   (e.g., sudden spikes or drops in demand that the model failed to anticipate).\n",
    "\n",
    "For each commodity, the test-set predictions from all seven models are plotted against the actual demand values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zxjf5Pc-EUDC",
    "outputId": "74248b18-795f-4ce8-e9ed-aebbf2144da5"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Actual vs. Forecast Visualisations\n",
    "# ============================================================\n",
    "\n",
    "def plot_actual_vs_forecast(commodity, output):\n",
    "    \"\"\"Plot actual vs predicted values for all seven models.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "    test_dates  = pd.to_datetime(output['test_dates'])\n",
    "    y_test      = output['y_test']\n",
    "    predictions = output['predictions']\n",
    "\n",
    "    # Actual values\n",
    "    ax.plot(test_dates, y_test, 'k-o', label='Actual',\n",
    "            linewidth=2, markersize=4, zorder=5)\n",
    "\n",
    "    # Model predictions\n",
    "    colors = {\n",
    "        'Random Forest': '#2196F3',\n",
    "        'XGBoost':       '#FF9800',\n",
    "        'SVR':           '#9C27B0',\n",
    "        'LSTM':          '#4CAF50',\n",
    "        'GRU':           '#E91E63',\n",
    "        'Prophet':       '#00BCD4',\n",
    "        'N-BEATS':       '#FF5722',\n",
    "    }\n",
    "    for model_name, pred in predictions.items():\n",
    "        if not np.all(np.isnan(pred)):\n",
    "            ax.plot(test_dates[:len(pred)], pred, '--', label=model_name,\n",
    "                    color=colors.get(model_name, 'gray'),\n",
    "                    linewidth=1.5, alpha=0.85)\n",
    "\n",
    "    ax.set_title(f'Actual vs. Forecast \\u2014 {commodity}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Demand Index')\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot for each commodity\n",
    "for commodity, output in all_crop_outputs.items():\n",
    "    plot_actual_vs_forecast(commodity, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c9PPkXAEUDC"
   },
   "source": [
    "### 5.2 Performance Comparison Table\n",
    "\n",
    "> **Understanding the four metrics:**\n",
    ">\n",
    "> | Metric | Full Name | What It Measures | Ideal Value |\n",
    "> |---|---|---|---|\n",
    "> | **RMSE** | Root Mean Squared Error | Average prediction error, penalising large errors more heavily (because of the squaring step). | 0 |\n",
    "> | **MAE** | Mean Absolute Error | Average absolute prediction error — more interpretable and less sensitive to outliers than RMSE. | 0 |\n",
    "> | **R²** | Coefficient of Determination | Proportion of variance in the actual values that is explained by the model (1.0 = perfect, 0 = no better than predicting the mean, negative = worse than the mean). | 1.0 |\n",
    "> | **MAPE** | Mean Absolute Percentage Error | Average error as a percentage of actual values — useful for comparing across commodities with different scales. | 0% |\n",
    ">\n",
    "> **How to read the heatmaps:**\n",
    "> Each cell shows the metric value for one commodity–model pair.\n",
    "> For RMSE, MAE, and MAPE, **green = good** (low error). For R², **green = good** (high explained variance).\n",
    "> The colour gradient makes it easy to spot which model dominates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "O0g_lY6ZEUDC",
    "outputId": "ffe9e665-1ca4-447a-ad39-3689567e2bb0"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 13: Performance Comparison Table & Heatmaps\n",
    "# ============================================================\n",
    "\n",
    "MODEL_ORDER = ['Random Forest', 'XGBoost', 'SVR', 'LSTM', 'GRU', 'Prophet', 'N-BEATS']\n",
    "\n",
    "# Build comparison DataFrame\n",
    "rows = []\n",
    "for commodity, models in all_results.items():\n",
    "    for model_name, metrics in models.items():\n",
    "        rows.append({\n",
    "            'Commodity': commodity,\n",
    "            'Model': model_name,\n",
    "            'RMSE': metrics['RMSE'],\n",
    "            'MAE':  metrics['MAE'],\n",
    "            'R²':   metrics['R²'],\n",
    "            'MAPE (%)': metrics['MAPE'],\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "# Pivot tables\n",
    "rmse_pivot = results_df.pivot(index='Commodity', columns='Model', values='RMSE')[MODEL_ORDER]\n",
    "mae_pivot  = results_df.pivot(index='Commodity', columns='Model', values='MAE')[MODEL_ORDER]\n",
    "r2_pivot   = results_df.pivot(index='Commodity', columns='Model', values='R²')[MODEL_ORDER]\n",
    "mape_pivot = results_df.pivot(index='Commodity', columns='Model', values='MAPE (%)')[MODEL_ORDER]\n",
    "\n",
    "# Print tables\n",
    "for name, pivot in [('RMSE', rmse_pivot), ('MAE', mae_pivot), ('R²', r2_pivot), ('MAPE (%)', mape_pivot)]:\n",
    "    direction = 'higher is better' if name == 'R²' else 'lower is better'\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f'{name} COMPARISON ({direction})')\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(pivot.to_string())\n",
    "\n",
    "# Heatmaps (2 x 2 grid)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(22, max(12, len(all_results) * 0.7)))\n",
    "\n",
    "heatmap_data = [\n",
    "    (rmse_pivot, 'RMSE by Commodity & Model\\n(lower = better)', '.2f', 'RdYlGn_r'),\n",
    "    (mae_pivot,  'MAE by Commodity & Model\\n(lower = better)',  '.2f', 'RdYlGn_r'),\n",
    "    (r2_pivot,   'R² by Commodity & Model\\n(higher = better)',  '.3f', 'RdYlGn'),\n",
    "    (mape_pivot, 'MAPE (%) by Commodity & Model\\n(lower = better)', '.1f', 'RdYlGn_r'),\n",
    "]\n",
    "\n",
    "for ax, (data, title, fmt, cmap) in zip(axes.flatten(), heatmap_data):\n",
    "    sns.heatmap(data, annot=True, fmt=fmt, cmap=cmap,\n",
    "                ax=ax, linewidths=0.5)\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QZC9NQOEUDD"
   },
   "source": [
    "### 5.3 Champion Model Selection\n",
    "\n",
    "> **What is a \"champion model\"?**\n",
    "> For each commodity, we pick the model with the **lowest RMSE** as the champion.\n",
    "> RMSE is used as the primary selection criterion because it penalises large errors\n",
    "> more than MAE, which is important in demand forecasting — a big prediction miss could\n",
    "> lead to significant food waste or stock-outs.\n",
    ">\n",
    "> The **Model Win Count** summary shows how many commodities each model \"won\", giving\n",
    "> an overall picture of which algorithm is most reliable across diverse crops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RG-2ki41EUDD",
    "outputId": "28bb6005-02ea-4134-f7fc-3c528d89ad17"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 14: Champion Model per Commodity\n",
    "# ============================================================\n",
    "\n",
    "print('=' * 70)\n",
    "print('CHAMPION MODEL PER COMMODITY (Lowest RMSE)')\n",
    "print('=' * 70)\n",
    "\n",
    "champion_rows = []\n",
    "for commodity, models in all_results.items():\n",
    "    best_model = None\n",
    "    best_rmse  = float('inf')\n",
    "\n",
    "    for model_name, metrics in models.items():\n",
    "        if not np.isnan(metrics['RMSE']) and metrics['RMSE'] < best_rmse:\n",
    "            best_rmse  = metrics['RMSE']\n",
    "            best_model = model_name\n",
    "\n",
    "    champion_rows.append({\n",
    "        'Commodity': commodity,\n",
    "        'Champion Model': best_model,\n",
    "        'RMSE': best_rmse,\n",
    "        'MAE':  models[best_model]['MAE']  if best_model else np.nan,\n",
    "        'R²':   models[best_model]['R²']   if best_model else np.nan,\n",
    "        'MAPE (%)': models[best_model]['MAPE'] if best_model else np.nan,\n",
    "    })\n",
    "\n",
    "champion_df = pd.DataFrame(champion_rows)\n",
    "print(champion_df.to_string(index=False))\n",
    "\n",
    "# Summary: which model wins most often?\n",
    "print(f\"\\n{chr(9472) * 70}\")\n",
    "print('MODEL WIN COUNT:')\n",
    "print(f\"{chr(9472) * 70}\")\n",
    "win_counts = champion_df['Champion Model'].value_counts()\n",
    "for model, count in win_counts.items():\n",
    "    pct = count / len(champion_df) * 100\n",
    "    print(f'  {model}: {count} crops ({pct:.0f}%)')\n",
    "\n",
    "overall_best = win_counts.index[0]\n",
    "print(f'\\nOverall Recommended Model: {overall_best}')\n",
    "print(f'  Champion for {win_counts.iloc[0]}/{len(champion_df)} commodities')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_AD-cAqEUDD"
   },
   "source": [
    "### 5.4 Feature Importance Analysis\n",
    "\n",
    "> **What is Feature Importance?**\n",
    "> Feature importance measures how much each input variable contributes to the model's predictions.\n",
    "> In a **Random Forest**, importance is calculated by measuring how much each feature reduces\n",
    "> prediction error (impurity) across all the trees in the forest.\n",
    ">\n",
    "> **How to read this chart:**\n",
    "> - Features with **longer bars** are more influential — the model relies on them heavily.\n",
    "> - Features with **short bars** contribute little; removing them might not hurt accuracy.\n",
    "> - This helps us understand *what drives demand* (e.g., is it mostly past demand,\n",
    ">   temperature, or the time of year?).\n",
    ">\n",
    "> We average the importance scores across all commodities to get a global view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 823
    },
    "id": "v5SEkjJgEUDD",
    "outputId": "f9621837-a2cf-41f0-aa8e-b91ed3d96413"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 15: Feature Importance (averaged from Random Forest)\n",
    "# ============================================================\n",
    "\n",
    "importance_dfs = []\n",
    "for commodity, output in all_crop_outputs.items():\n",
    "    fi = output['feature_importance']\n",
    "    fi_df = pd.DataFrame([fi])\n",
    "    fi_df['Commodity'] = commodity\n",
    "    importance_dfs.append(fi_df)\n",
    "\n",
    "if importance_dfs:\n",
    "    all_importance = pd.concat(importance_dfs, ignore_index=True)\n",
    "    avg_importance = all_importance[FEATURE_COLS].mean().sort_values(ascending=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    avg_importance.plot(kind='barh', ax=ax, color='steelblue')\n",
    "    ax.set_title('Average Feature Importance Across All Commodities (Random Forest)',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('\\nTop 5 most important features:')\n",
    "    for feat, imp in avg_importance.tail(5).items():\n",
    "        print(f'  {feat}: {imp:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "182EhNwPEUDE"
   },
   "source": [
    "## 6. Export Results\n",
    "\n",
    "This section exports:\n",
    "1. **`model_comparison_results.csv`** — Full metrics (RMSE, MAE, R², MAPE) for every commodity × model combination\n",
    "2. **`champion_models.csv`** — The best model per commodity with its metrics\n",
    "3. **`models/` directory** — Trained champion models (`.pkl` for sklearn, `.keras` for neural nets) + metadata JSON for the web prediction UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "wjZq9wz0EUDE",
    "outputId": "3015b38d-e25e-4fa5-9cf3-addbdf7923d1"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 16: Export Results\n",
    "# ============================================================\n",
    "\n",
    "# Save metric comparison tables\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "champion_df.to_csv('champion_models.csv', index=False)\n",
    "\n",
    "# -- Save trained champion models for the web prediction UI --\n",
    "import pickle\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "export_data = {\n",
    "    'feature_cols': FEATURE_COLS,\n",
    "    'target_col': TARGET_COL,\n",
    "    'commodity_stats': {},\n",
    "}\n",
    "\n",
    "for commodity, output in all_crop_outputs.items():\n",
    "    champ_row = champion_df[champion_df['Commodity'] == commodity]\n",
    "    if champ_row.empty:\n",
    "        continue\n",
    "    champ_name = champ_row.iloc[0]['Champion Model']\n",
    "    model_obj  = output['trained_models'].get(champ_name)\n",
    "    if model_obj is None:\n",
    "        continue\n",
    "\n",
    "    export_data['commodity_stats'][commodity] = {\n",
    "        'champion_model': champ_name,\n",
    "        'metrics': output['results'][champ_name],\n",
    "        'train_size': output['train_size'],\n",
    "        'test_size': output['test_size'],\n",
    "    }\n",
    "\n",
    "    # Save the model (sklearn models -> joblib, keras models -> keras format)\n",
    "    safe_name = commodity.replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_')\n",
    "    if champ_name in ('LSTM', 'GRU', 'N-BEATS'):\n",
    "        model_obj.save(f'models/{safe_name}_model.keras')\n",
    "    elif champ_name == 'SVR':\n",
    "        joblib.dump(model_obj, f'models/{safe_name}_model.pkl')  # dict with model + scalers\n",
    "    else:\n",
    "        joblib.dump(model_obj, f'models/{safe_name}_model.pkl')\n",
    "\n",
    "    # Save scalers for the commodity (needed for prediction)\n",
    "    if 'scalers' in output:\n",
    "        joblib.dump(output['scalers'], f'models/{safe_name}_scalers.pkl')\n",
    "\n",
    "# Save the metadata\n",
    "with open('models/metadata.json', 'w') as f:\n",
    "    _json.dump(export_data, f, indent=2, default=str)\n",
    "\n",
    "# Save all_results for the dashboard\n",
    "with open('models/all_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print('Results saved:')\n",
    "print('  - model_comparison_results.csv  (full metrics per commodity per model)')\n",
    "print('  - champion_models.csv           (best model per commodity)')\n",
    "print('  - models/                       (trained champion models + metadata)')\n",
    "\n",
    "# Download files\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "files.download('model_comparison_results.csv')\n",
    "files.download('champion_models.csv')\n",
    "\n",
    "# Zip the models folder and download it\n",
    "shutil.make_archive('models', 'zip', '.', 'models')\n",
    "files.download('models.zip')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}