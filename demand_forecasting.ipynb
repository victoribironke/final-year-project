{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"},
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predictive Demand Forecasting Using Machine Learning to Reduce Food Waste\n",
        "## Chapter 4: Results and Discussion\n",
        "\n",
        "**Models Compared:**\n",
        "- **Random Forest (RF)** \u2014 Robust ensemble baseline\n",
        "- **XGBoost** \u2014 Gradient-boosted trees, optimised for speed\n",
        "- **LSTM** \u2014 Deep Learning RNN for complex time-series patterns\n",
        "\n",
        "**Evaluation Metrics:** RMSE, MAPE\n",
        "\n",
        "**Strategy:** Loop through every unique commodity, train all three models, identify the champion model per crop.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 1: Install Required Packages\n",
        "# ============================================================\n",
        "!pip install -q xgboost scikit-learn pandas numpy matplotlib seaborn tensorflow\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print('All packages installed successfully.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 2: Import Libraries\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# TensorFlow / Keras (LSTM)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Plotting settings\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(f'TensorFlow version: {tf.__version__}')\n",
        "print(f'NumPy version: {np.__version__}')\n",
        "print(f'Pandas version: {pd.__version__}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 3: Upload and Load Data\n",
        "# ============================================================\n",
        "from google.colab import files\n",
        "\n",
        "print('Please upload your harmonized CSV file:')\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the first uploaded file\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(filename, parse_dates=['Date'])\n",
        "\n",
        "print(f\"\\nLoaded '{filename}' with {len(df):,} rows and {len(df.columns)} columns.\")\n",
        "print(f'\\nColumns: {list(df.columns)}')\n",
        "print(f\"\\nDate range: {df['Date'].min()} to {df['Date'].max()}\")\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 4: Dataset Overview\n",
        "# ============================================================\n",
        "print('=' * 60)\n",
        "print('DATASET OVERVIEW')\n",
        "print('=' * 60)\n",
        "\n",
        "print(f'\\nShape: {df.shape}')\n",
        "print(f'\\nData Types:')\n",
        "print(df.dtypes)\n",
        "print(f'\\nMissing Values:')\n",
        "print(df.isnull().sum())\n",
        "print(f'\\nBasic Statistics:')\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 5: Commodity Distribution & Demand Overview\n",
        "# ============================================================\n",
        "commodity_counts = df['Commodity_Name'].value_counts()\n",
        "print(f'Unique Commodities: {len(commodity_counts)}\\n')\n",
        "print(commodity_counts.to_string())\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Top 15 commodities by record count\n",
        "commodity_counts.head(15).plot(kind='barh', ax=axes[0], color='steelblue')\n",
        "axes[0].set_title('Top 15 Commodities by Record Count')\n",
        "axes[0].set_xlabel('Number of Records')\n",
        "\n",
        "# Demand distribution\n",
        "df['Demand'].hist(bins=50, ax=axes[1], color='coral', edgecolor='black')\n",
        "axes[1].set_title('Distribution of Demand (Proxy)')\n",
        "axes[1].set_xlabel('Demand Index (0-100)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n",
        "\n",
        "Steps:\n",
        "1. Aggregate data per **(Date, Commodity)** across all markets \u2014 creates one time series per commodity\n",
        "2. Handle missing values (forward-fill within groups, then drop remaining)\n",
        "\n",
        "> **Note:** `Unit_Price` is excluded as a feature since `Demand` is derived from it (avoids data leakage)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 6: Aggregate by Date & Commodity\n",
        "# ============================================================\n",
        "# Aggregate across all markets for each (Date, Commodity) pair.\n",
        "# This gives us one clean time series per commodity.\n",
        "df_agg = df.groupby(['Date', 'Commodity_Name']).agg({\n",
        "    'Demand': 'mean',\n",
        "    'Unit_Price': 'mean',        # kept for reference only, NOT used as feature\n",
        "    'Avg_Temperature': 'mean',\n",
        "    'Rainfall': 'mean',\n",
        "    'Is_Holiday': 'max'           # 1 if any market flagged it as holiday\n",
        "}).reset_index()\n",
        "\n",
        "df_agg = df_agg.sort_values(['Commodity_Name', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# Ensure Date is datetime after groupby\n",
        "df_agg['Date'] = pd.to_datetime(df_agg['Date'])\n",
        "\n",
        "# Forward-fill missing weather data within each commodity group\n",
        "df_agg['Avg_Temperature'] = df_agg.groupby('Commodity_Name')['Avg_Temperature'].transform(\n",
        "    lambda x: x.ffill().bfill()\n",
        ")\n",
        "df_agg['Rainfall'] = df_agg.groupby('Commodity_Name')['Rainfall'].transform(\n",
        "    lambda x: x.ffill().bfill()\n",
        ")\n",
        "\n",
        "print(f'Aggregated dataset: {len(df_agg):,} rows')\n",
        "print(f\"Unique commodities: {df_agg['Commodity_Name'].nunique()}\")\n",
        "print(f'\\nRecords per commodity:')\n",
        "print(df_agg.groupby('Commodity_Name').size().describe())\n",
        "print(f'\\nRemaining missing values:')\n",
        "print(df_agg.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering\n",
        "\n",
        "Creating temporal features for time-series forecasting:\n",
        "- **Lag Features**: Demand at *t\u22121*, *t\u22123*, *t\u22126*, *t\u221212* months\n",
        "- **Rolling Means**: 3-month, 6-month, 12-month moving averages of demand\n",
        "- **Date Features**: Month (cyclical sin/cos encoding), Year\n",
        "- **Seasonal**: Nigerian wet/dry season indicator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 7: Feature Engineering Functions\n",
        "# ============================================================\n",
        "\n",
        "def create_features(group_df):\n",
        "    \"\"\"\n",
        "    Create lag features, rolling means, and date features for a single commodity.\n",
        "    Designed for monthly data (each lag unit = 1 month).\n",
        "    \"\"\"\n",
        "    df = group_df.copy()\n",
        "\n",
        "    # --- Lag Features (monthly data) ---\n",
        "    df['Demand_Lag1']  = df['Demand'].shift(1)    # Previous month\n",
        "    df['Demand_Lag3']  = df['Demand'].shift(3)    # 3 months ago\n",
        "    df['Demand_Lag6']  = df['Demand'].shift(6)    # 6 months ago\n",
        "    df['Demand_Lag12'] = df['Demand'].shift(12)   # Same month last year\n",
        "\n",
        "    # --- Rolling Mean Features ---\n",
        "    df['Demand_RollingMean3']  = df['Demand'].rolling(window=3).mean()    # 3-month avg\n",
        "    df['Demand_RollingMean6']  = df['Demand'].rolling(window=6).mean()    # 6-month avg\n",
        "    df['Demand_RollingMean12'] = df['Demand'].rolling(window=12).mean()   # 12-month avg\n",
        "\n",
        "    # --- Date Features ---\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Year']  = df['Date'].dt.year\n",
        "\n",
        "    # Cyclical encoding of month (captures Jan-Dec continuity)\n",
        "    df['Month_Sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "    df['Month_Cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "\n",
        "    # --- Nigerian Seasons ---\n",
        "    # Wet season: April-October, Dry season: November-March\n",
        "    df['Is_Wet_Season'] = df['Month'].apply(lambda m: 1 if 4 <= m <= 10 else 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Feature columns the models will train on\n",
        "FEATURE_COLS = [\n",
        "    'Demand_Lag1', 'Demand_Lag3', 'Demand_Lag6', 'Demand_Lag12',\n",
        "    'Demand_RollingMean3', 'Demand_RollingMean6', 'Demand_RollingMean12',\n",
        "    'Avg_Temperature', 'Rainfall', 'Is_Holiday',\n",
        "    'Month_Sin', 'Month_Cos', 'Is_Wet_Season', 'Year'\n",
        "]\n",
        "\n",
        "TARGET_COL = 'Demand'\n",
        "\n",
        "# Minimum samples required to train models for a commodity\n",
        "MIN_SAMPLES = 36   # At least 3 years of monthly data\n",
        "\n",
        "print(f'Feature columns ({len(FEATURE_COLS)}): {FEATURE_COLS}')\n",
        "print(f'Target: {TARGET_COL}')\n",
        "print(f'Minimum samples: {MIN_SAMPLES}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 8: Apply Feature Engineering to All Commodities\n",
        "# ============================================================\n",
        "df_features = df_agg.groupby('Commodity_Name', group_keys=False).apply(create_features)\n",
        "\n",
        "# Check which commodities have enough data after dropping NaN rows\n",
        "commodity_sample_counts = {}\n",
        "for commodity in df_features['Commodity_Name'].unique():\n",
        "    crop_data = df_features[df_features['Commodity_Name'] == commodity].dropna(subset=FEATURE_COLS)\n",
        "    commodity_sample_counts[commodity] = len(crop_data)\n",
        "\n",
        "valid_commodities = [c for c, n in commodity_sample_counts.items() if n >= MIN_SAMPLES]\n",
        "skipped_commodities = [c for c, n in commodity_sample_counts.items() if n < MIN_SAMPLES]\n",
        "\n",
        "print(f'Commodities with enough data (>= {MIN_SAMPLES} samples): {len(valid_commodities)}')\n",
        "print(f'Commodities skipped (too few samples): {len(skipped_commodities)}')\n",
        "\n",
        "if skipped_commodities:\n",
        "    print(f'\\nSkipped: {skipped_commodities}')\n",
        "print(f'\\nWill train models for:')\n",
        "for c in valid_commodities:\n",
        "    print(f'  - {c} ({commodity_sample_counts[c]} samples)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Training\n",
        "\n",
        "For each valid commodity:\n",
        "1. **Prepare data**: Extract features, chronological 80/20 train/test split\n",
        "2. **Train Random Forest**: 200 trees, max depth 10\n",
        "3. **Train XGBoost**: 200 rounds, learning rate 0.1\n",
        "4. **Train LSTM**: 2-layer LSTM (64 + 32 units), 12-month lookback window\n",
        "5. **Evaluate**: Compute RMSE and MAPE for each model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 9: Model Helper Functions\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    \"\"\"Calculate RMSE and MAPE.\"\"\"\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    # Avoid division by zero in MAPE\n",
        "    mask = y_true != 0\n",
        "    if mask.sum() > 0:\n",
        "        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "    else:\n",
        "        mape = np.nan\n",
        "    return {'RMSE': round(rmse, 4), 'MAPE': round(mape, 2)}\n",
        "\n",
        "\n",
        "def build_lstm_model(n_features, lookback):\n",
        "    \"\"\"Build an LSTM model for time-series forecasting.\"\"\"\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=(lookback, n_features)),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_lstm_sequences(X, y, lookback=12):\n",
        "    \"\"\"Create sliding-window sequences for LSTM input.\"\"\"\n",
        "    Xs, ys = [], []\n",
        "    for i in range(lookback, len(X)):\n",
        "        Xs.append(X[i - lookback:i])\n",
        "        ys.append(y[i])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "print('Helper functions defined.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 10: Per-Crop Training Function\n",
        "# ============================================================\n",
        "\n",
        "def train_and_evaluate_crop(commodity_name, df_features, feature_cols, target_col,\n",
        "                            test_size=0.2, lstm_lookback=12):\n",
        "    \"\"\"\n",
        "    Train RF, XGBoost, and LSTM models for a single commodity.\n",
        "\n",
        "    Returns dict with model results, predictions, and test data for plotting.\n",
        "    \"\"\"\n",
        "    # Filter and prepare data\n",
        "    crop_data = df_features[df_features['Commodity_Name'] == commodity_name].copy()\n",
        "    crop_data = crop_data.dropna(subset=feature_cols + [target_col])\n",
        "    crop_data = crop_data.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    if len(crop_data) < MIN_SAMPLES:\n",
        "        return None\n",
        "\n",
        "    # Chronological train/test split\n",
        "    split_idx = int(len(crop_data) * (1 - test_size))\n",
        "    train_data = crop_data.iloc[:split_idx]\n",
        "    test_data  = crop_data.iloc[split_idx:]\n",
        "\n",
        "    X_train = train_data[feature_cols].values\n",
        "    y_train = train_data[target_col].values\n",
        "    X_test  = test_data[feature_cols].values\n",
        "    y_test  = test_data[target_col].values\n",
        "    test_dates = test_data['Date'].values\n",
        "\n",
        "    results     = {}\n",
        "    predictions = {}\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # 1. RANDOM FOREST\n",
        "    # ----------------------------------------------------------------\n",
        "    rf_model = RandomForestRegressor(\n",
        "        n_estimators=200, max_depth=10,\n",
        "        min_samples_split=5, random_state=42, n_jobs=-1\n",
        "    )\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_pred = rf_model.predict(X_test)\n",
        "    results['Random Forest'] = evaluate_model(y_test, rf_pred)\n",
        "    predictions['Random Forest'] = rf_pred\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # 2. XGBOOST\n",
        "    # ----------------------------------------------------------------\n",
        "    xgb_model = XGBRegressor(\n",
        "        n_estimators=200, max_depth=6, learning_rate=0.1,\n",
        "        subsample=0.8, colsample_bytree=0.8,\n",
        "        random_state=42, verbosity=0\n",
        "    )\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    xgb_pred = xgb_model.predict(X_test)\n",
        "    results['XGBoost'] = evaluate_model(y_test, xgb_pred)\n",
        "    predictions['XGBoost'] = xgb_pred\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # 3. LSTM\n",
        "    # ----------------------------------------------------------------\n",
        "    scaler_X = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "\n",
        "    X_all = crop_data[feature_cols].values\n",
        "    y_all = crop_data[target_col].values.reshape(-1, 1)\n",
        "\n",
        "    X_scaled = scaler_X.fit_transform(X_all)\n",
        "    y_scaled = scaler_y.fit_transform(y_all).flatten()\n",
        "\n",
        "    # Create sequences\n",
        "    X_seq, y_seq = create_lstm_sequences(X_scaled, y_scaled, lookback=lstm_lookback)\n",
        "\n",
        "    if len(X_seq) < MIN_SAMPLES:\n",
        "        results['LSTM'] = {'RMSE': np.nan, 'MAPE': np.nan}\n",
        "        predictions['LSTM'] = np.full(len(y_test), np.nan)\n",
        "    else:\n",
        "        # Split sequences (accounting for lookback offset)\n",
        "        seq_split = split_idx - lstm_lookback\n",
        "        if seq_split <= 0 or seq_split >= len(X_seq):\n",
        "            results['LSTM'] = {'RMSE': np.nan, 'MAPE': np.nan}\n",
        "            predictions['LSTM'] = np.full(len(y_test), np.nan)\n",
        "        else:\n",
        "            X_train_seq = X_seq[:seq_split]\n",
        "            y_train_seq = y_seq[:seq_split]\n",
        "            X_test_seq  = X_seq[seq_split:]\n",
        "            y_test_seq  = y_seq[seq_split:]\n",
        "\n",
        "            # Build and train\n",
        "            lstm_model = build_lstm_model(len(feature_cols), lstm_lookback)\n",
        "\n",
        "            early_stop = EarlyStopping(\n",
        "                monitor='val_loss', patience=10, restore_best_weights=True\n",
        "            )\n",
        "\n",
        "            lstm_model.fit(\n",
        "                X_train_seq, y_train_seq,\n",
        "                epochs=100, batch_size=16,\n",
        "                validation_split=0.15,\n",
        "                callbacks=[early_stop],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Predict and inverse-transform\n",
        "            lstm_pred_scaled = lstm_model.predict(X_test_seq, verbose=0).flatten()\n",
        "            lstm_pred = scaler_y.inverse_transform(\n",
        "                lstm_pred_scaled.reshape(-1, 1)\n",
        "            ).flatten()\n",
        "            y_test_lstm = scaler_y.inverse_transform(\n",
        "                y_test_seq.reshape(-1, 1)\n",
        "            ).flatten()\n",
        "\n",
        "            results['LSTM'] = evaluate_model(y_test_lstm, lstm_pred)\n",
        "\n",
        "            # Pad LSTM predictions to match test set length\n",
        "            lstm_full = np.full(len(y_test), np.nan)\n",
        "            lstm_full[-len(lstm_pred):] = lstm_pred\n",
        "            predictions['LSTM'] = lstm_full\n",
        "\n",
        "    return {\n",
        "        'results': results,\n",
        "        'predictions': predictions,\n",
        "        'y_test': y_test,\n",
        "        'test_dates': test_dates,\n",
        "        'train_size': len(train_data),\n",
        "        'test_size': len(test_data),\n",
        "        'feature_importance': dict(zip(feature_cols, rf_model.feature_importances_))\n",
        "    }\n",
        "\n",
        "print('Training function defined.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 11: Run the Multi-Crop Training Loop\n",
        "# ============================================================\n",
        "\n",
        "all_results = {}\n",
        "all_crop_outputs = {}\n",
        "\n",
        "print('=' * 70)\n",
        "print('MULTI-CROP MODEL TRAINING')\n",
        "print('=' * 70)\n",
        "\n",
        "for i, commodity in enumerate(valid_commodities):\n",
        "    print(f\"\\n{'─' * 70}\")\n",
        "    print(f'[{i+1}/{len(valid_commodities)}] Training models for: {commodity}')\n",
        "    print(f\"{'─' * 70}\")\n",
        "\n",
        "    output = train_and_evaluate_crop(\n",
        "        commodity_name=commodity,\n",
        "        df_features=df_features,\n",
        "        feature_cols=FEATURE_COLS,\n",
        "        target_col=TARGET_COL\n",
        "    )\n",
        "\n",
        "    if output is None:\n",
        "        print(f'  Skipped (insufficient data)')\n",
        "        continue\n",
        "\n",
        "    all_crop_outputs[commodity] = output\n",
        "    all_results[commodity] = output['results']\n",
        "\n",
        "    # Print results for this commodity\n",
        "    print(f\"  Train: {output['train_size']} samples | Test: {output['test_size']} samples\")\n",
        "    for model_name, metrics in output['results'].items():\n",
        "        print(f\"  {model_name:15s} -> RMSE: {metrics['RMSE']:8.4f} | MAPE: {metrics['MAPE']:6.2f}%\")\n",
        "\n",
        "print(f\"\\n{'=' * 70}\")\n",
        "print(f'TRAINING COMPLETE: {len(all_results)} commodities processed')\n",
        "print(f\"{'=' * 70}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Results & Visualisation\n",
        "### 5.1 Actual vs. Forecast Plots\n",
        "For each commodity, the test-set predictions from all three models are plotted against the actual demand values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 12: Actual vs. Forecast Visualisations\n",
        "# ============================================================\n",
        "\n",
        "def plot_actual_vs_forecast(commodity, output):\n",
        "    \"\"\"Plot actual vs predicted values for all three models.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(14, 5))\n",
        "\n",
        "    test_dates  = pd.to_datetime(output['test_dates'])\n",
        "    y_test      = output['y_test']\n",
        "    predictions = output['predictions']\n",
        "\n",
        "    # Actual values\n",
        "    ax.plot(test_dates, y_test, 'k-o', label='Actual',\n",
        "            linewidth=2, markersize=4, zorder=5)\n",
        "\n",
        "    # Model predictions\n",
        "    colors = {'Random Forest': '#2196F3', 'XGBoost': '#FF9800', 'LSTM': '#4CAF50'}\n",
        "    for model_name, pred in predictions.items():\n",
        "        if not np.all(np.isnan(pred)):\n",
        "            ax.plot(test_dates, pred, '--', label=model_name,\n",
        "                    color=colors.get(model_name, 'gray'),\n",
        "                    linewidth=1.5, alpha=0.85)\n",
        "\n",
        "    ax.set_title(f'Actual vs. Forecast \\u2014 {commodity}',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.set_ylabel('Demand Index')\n",
        "    ax.legend(loc='best')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot for each commodity\n",
        "for commodity, output in all_crop_outputs.items():\n",
        "    plot_actual_vs_forecast(commodity, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Performance Comparison Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 13: Performance Comparison Table & Heatmaps\n",
        "# ============================================================\n",
        "\n",
        "# Build comparison DataFrame\n",
        "rows = []\n",
        "for commodity, models in all_results.items():\n",
        "    for model_name, metrics in models.items():\n",
        "        rows.append({\n",
        "            'Commodity': commodity,\n",
        "            'Model': model_name,\n",
        "            'RMSE': metrics['RMSE'],\n",
        "            'MAPE (%)': metrics['MAPE']\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(rows)\n",
        "\n",
        "# Pivot tables\n",
        "rmse_pivot = results_df.pivot(index='Commodity', columns='Model', values='RMSE')\n",
        "rmse_pivot = rmse_pivot[['Random Forest', 'XGBoost', 'LSTM']]\n",
        "\n",
        "mape_pivot = results_df.pivot(index='Commodity', columns='Model', values='MAPE (%)')\n",
        "mape_pivot = mape_pivot[['Random Forest', 'XGBoost', 'LSTM']]\n",
        "\n",
        "print('=' * 70)\n",
        "print('RMSE COMPARISON (lower is better)')\n",
        "print('=' * 70)\n",
        "print(rmse_pivot.to_string())\n",
        "\n",
        "print(f\"\\n{'=' * 70}\")\n",
        "print('MAPE (%) COMPARISON (lower is better)')\n",
        "print('=' * 70)\n",
        "print(mape_pivot.to_string())\n",
        "\n",
        "# Heatmaps\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, max(6, len(all_results) * 0.5)))\n",
        "\n",
        "sns.heatmap(rmse_pivot, annot=True, fmt='.2f', cmap='RdYlGn_r',\n",
        "            ax=axes[0], linewidths=0.5)\n",
        "axes[0].set_title('RMSE by Commodity & Model', fontsize=13, fontweight='bold')\n",
        "\n",
        "sns.heatmap(mape_pivot, annot=True, fmt='.1f', cmap='RdYlGn_r',\n",
        "            ax=axes[1], linewidths=0.5)\n",
        "axes[1].set_title('MAPE (%) by Commodity & Model', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Champion Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 14: Champion Model per Commodity\n",
        "# ============================================================\n",
        "\n",
        "print('=' * 70)\n",
        "print('CHAMPION MODEL PER COMMODITY (Lowest RMSE)')\n",
        "print('=' * 70)\n",
        "\n",
        "champion_rows = []\n",
        "for commodity, models in all_results.items():\n",
        "    best_model = None\n",
        "    best_rmse  = float('inf')\n",
        "\n",
        "    for model_name, metrics in models.items():\n",
        "        if not np.isnan(metrics['RMSE']) and metrics['RMSE'] < best_rmse:\n",
        "            best_rmse  = metrics['RMSE']\n",
        "            best_model = model_name\n",
        "\n",
        "    champion_rows.append({\n",
        "        'Commodity': commodity,\n",
        "        'Champion Model': best_model,\n",
        "        'RMSE': best_rmse,\n",
        "        'MAPE (%)': models[best_model]['MAPE'] if best_model else np.nan\n",
        "    })\n",
        "\n",
        "champion_df = pd.DataFrame(champion_rows)\n",
        "print(champion_df.to_string(index=False))\n",
        "\n",
        "# Summary: which model wins most often?\n",
        "print(f\"\\n{'─' * 70}\")\n",
        "print('MODEL WIN COUNT:')\n",
        "print(f\"{'─' * 70}\")\n",
        "win_counts = champion_df['Champion Model'].value_counts()\n",
        "for model, count in win_counts.items():\n",
        "    pct = count / len(champion_df) * 100\n",
        "    print(f'  {model}: {count} crops ({pct:.0f}%)')\n",
        "\n",
        "overall_best = win_counts.index[0]\n",
        "print(f'\\nOverall Recommended Model: {overall_best}')\n",
        "print(f'  Champion for {win_counts.iloc[0]}/{len(champion_df)} commodities')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 15: Feature Importance (averaged from Random Forest)\n",
        "# ============================================================\n",
        "\n",
        "importance_dfs = []\n",
        "for commodity, output in all_crop_outputs.items():\n",
        "    fi = output['feature_importance']\n",
        "    fi_df = pd.DataFrame([fi])\n",
        "    fi_df['Commodity'] = commodity\n",
        "    importance_dfs.append(fi_df)\n",
        "\n",
        "if importance_dfs:\n",
        "    all_importance = pd.concat(importance_dfs, ignore_index=True)\n",
        "    avg_importance = all_importance[FEATURE_COLS].mean().sort_values(ascending=True)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 7))\n",
        "    avg_importance.plot(kind='barh', ax=ax, color='steelblue')\n",
        "    ax.set_title('Average Feature Importance Across All Commodities (Random Forest)',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print('\\nTop 5 most important features:')\n",
        "    for feat, imp in avg_importance.tail(5).items():\n",
        "        print(f'  {feat}: {imp:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Export Results for Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Cell 16: Export Results\n",
        "# ============================================================\n",
        "\n",
        "# Save to CSV\n",
        "results_df.to_csv('model_comparison_results.csv', index=False)\n",
        "champion_df.to_csv('champion_models.csv', index=False)\n",
        "\n",
        "print('Results saved:')\n",
        "print('  - model_comparison_results.csv  (full RMSE & MAPE per commodity per model)')\n",
        "print('  - champion_models.csv           (best model per commodity)')\n",
        "\n",
        "# Download files\n",
        "from google.colab import files\n",
        "files.download('model_comparison_results.csv')\n",
        "files.download('champion_models.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
